{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Atlas Transformer for Microsoft Purview\n",
        "\n",
        "This notebook transforms extracted metadata into Apache Atlas format for Purview ingestion.\n",
        "\n",
        "**Parameters:**\n",
        "- `metadata_path`: Path to extracted metadata JSON file\n",
        "- `source_type`: Type of data source\n",
        "- `collection_name`: Purview collection name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from notebookutils import mssparkutils\n",
        "from datetime import datetime\n",
        "import json\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get parameters\n",
        "metadata_path = mssparkutils.notebook.getArgument(\"metadata_path\", \"\")\n",
        "source_type = mssparkutils.notebook.getArgument(\"source_type\", \"sql_server\")\n",
        "collection_name = mssparkutils.notebook.getArgument(\"collection_name\", \"default\")\n",
        "\n",
        "logger.info(f\"Metadata Path: {metadata_path}\")\n",
        "logger.info(f\"Source Type: {source_type}\")\n",
        "logger.info(f\"Collection: {collection_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load metadata\n",
        "logger.info(f\"Loading metadata from {metadata_path}...\")\n",
        "\n",
        "with open(metadata_path, 'r') as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "logger.info(\"Metadata loaded successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import SDK\n",
        "import sys\n",
        "sys.path.append(\"/lakehouse/default/Files/libs\")\n",
        "\n",
        "from purview_connector_sdk import DatabaseConnector, FileSystemConnector, PurviewClient\n",
        "\n",
        "# Create mock client for transformation only\n",
        "purview_client = PurviewClient(\n",
        "    account_name=\"mock\",\n",
        "    use_managed_identity=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create connector based on source type\n",
        "if source_type == \"sql_server\":\n",
        "    connector = DatabaseConnector(\n",
        "        purview_client=purview_client,\n",
        "        source_type=source_type,\n",
        "        connection_string=\"mock\",\n",
        "        collection_name=collection_name\n",
        "    )\n",
        "elif source_type == \"file_system\":\n",
        "    connector = FileSystemConnector(\n",
        "        purview_client=purview_client,\n",
        "        root_path=\"mock\",\n",
        "        collection_name=collection_name\n",
        "    )\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported source type: {source_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transform to Atlas format\n",
        "logger.info(\"Transforming metadata to Atlas format...\")\n",
        "\n",
        "atlas_entities = connector.transform_to_atlas(metadata)\n",
        "\n",
        "logger.info(f\"Transformed {len(atlas_entities)} entities\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample entities\n",
        "print(\"=\" * 60)\n",
        "print(\"Sample Atlas Entities\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for i, entity in enumerate(atlas_entities[:3]):  # Show first 3\n",
        "    print(f\"\\nEntity {i+1}:\")\n",
        "    print(f\"  Type: {entity['typeName']}\")\n",
        "    print(f\"  Name: {entity['attributes'].get('name')}\")\n",
        "    print(f\"  QualifiedName: {entity['attributes'].get('qualifiedName')}\")\n",
        "\n",
        "print(f\"\\n... and {len(atlas_entities) - 3} more entities\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate entities\n",
        "logger.info(\"Validating Atlas entities...\")\n",
        "\n",
        "try:\n",
        "    connector.validate_entities(atlas_entities)\n",
        "    logger.info(\"✓ All entities are valid\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"✗ Validation failed: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare Atlas JSON for Purview\n",
        "atlas_json = {\n",
        "    \"entities\": atlas_entities,\n",
        "    \"referredEntities\": {},\n",
        "    \"metadata\": {\n",
        "        \"source_type\": source_type,\n",
        "        \"collection\": collection_name,\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"entity_count\": len(atlas_entities)\n",
        "    }\n",
        "}\n",
        "\n",
        "logger.info(f\"Prepared Atlas JSON with {len(atlas_entities)} entities\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save Atlas JSON to processed folder\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "output_path = f\"/lakehouse/default/Files/processed/atlas_{source_type}_{timestamp}.json\"\n",
        "\n",
        "logger.info(f\"Saving Atlas JSON to: {output_path}\")\n",
        "\n",
        "with open(output_path, 'w') as f:\n",
        "    json.dump(atlas_json, f, indent=2, default=str)\n",
        "\n",
        "logger.info(\"Atlas JSON saved successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save transformation log\n",
        "log_path = f\"/lakehouse/default/Files/logs/transformation_{timestamp}.log\"\n",
        "\n",
        "log_entry = {\n",
        "    \"timestamp\": timestamp,\n",
        "    \"source_type\": source_type,\n",
        "    \"metadata_path\": metadata_path,\n",
        "    \"atlas_json_path\": output_path,\n",
        "    \"entity_count\": len(atlas_entities),\n",
        "    \"status\": \"success\"\n",
        "}\n",
        "\n",
        "with open(log_path, 'w') as f:\n",
        "    json.dump(log_entry, f, indent=2)\n",
        "\n",
        "logger.info(f\"Log saved to: {log_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Return output for next activity\n",
        "output = {\n",
        "    \"status\": \"success\",\n",
        "    \"atlas_json_path\": output_path,\n",
        "    \"log_path\": log_path,\n",
        "    \"entity_count\": len(atlas_entities),\n",
        "    \"timestamp\": timestamp\n",
        "}\n",
        "\n",
        "logger.info(\"Atlas transformation complete\")\n",
        "logger.info(f\"Output: {json.dumps(output, indent=2)}\")\n",
        "\n",
        "mssparkutils.notebook.exit(json.dumps(output))"
      ]
    }
  ]
}
