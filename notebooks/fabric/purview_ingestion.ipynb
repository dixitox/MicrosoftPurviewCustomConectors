{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Purview Ingestion Notebook\n",
        "\n",
        "This notebook ingests Atlas entities into Microsoft Purview.\n",
        "\n",
        "**Parameters:**\n",
        "- `atlas_json_path`: Path to Atlas JSON file\n",
        "- `purview_endpoint`: Purview endpoint URL (from Key Vault)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "from notebookutils import mssparkutils\n",
        "from datetime import datetime\n",
        "import json\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get parameters\n",
        "atlas_json_path = mssparkutils.notebook.getArgument(\"atlas_json_path\", \"\")\n",
        "\n",
        "logger.info(f\"Atlas JSON Path: {atlas_json_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Atlas JSON\n",
        "logger.info(f\"Loading Atlas JSON from {atlas_json_path}...\")\n",
        "\n",
        "with open(atlas_json_path, 'r') as f:\n",
        "    atlas_data = json.load(f)\n",
        "\n",
        "entities = atlas_data.get('entities', [])\n",
        "metadata = atlas_data.get('metadata', {})\n",
        "\n",
        "logger.info(f\"Loaded {len(entities)} entities\")\n",
        "logger.info(f\"Source Type: {metadata.get('source_type')}\")\n",
        "logger.info(f\"Collection: {metadata.get('collection')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get Purview credentials from Key Vault\n",
        "logger.info(\"Retrieving Purview credentials...\")\n",
        "\n",
        "try:\n",
        "    purview_endpoint = mssparkutils.credentials.getSecret(\n",
        "        \"purview-connector-kv\",\n",
        "        \"purview-endpoint\"\n",
        "    )\n",
        "    logger.info(f\"Purview endpoint: {purview_endpoint}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Failed to get Purview endpoint from Key Vault: {e}\")\n",
        "    purview_endpoint = \"https://your-purview.purview.azure.com\"\n",
        "    logger.warning(f\"Using default endpoint: {purview_endpoint}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Purview client\n",
        "import sys\n",
        "sys.path.append(\"/lakehouse/default/Files/libs\")\n",
        "\n",
        "from purview_connector_sdk import PurviewClient\n",
        "\n",
        "logger.info(\"Initializing Purview client with Managed Identity...\")\n",
        "\n",
        "purview_client = PurviewClient(\n",
        "    endpoint=purview_endpoint,\n",
        "    use_managed_identity=True\n",
        ")\n",
        "\n",
        "logger.info(\"✓ Purview client initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test Purview connection\n",
        "try:\n",
        "    account_info = purview_client.get_account_info()\n",
        "    logger.info(f\"✓ Connected to Purview: {account_info.get('name')}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"✗ Failed to connect to Purview: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ingest entities in batches\n",
        "logger.info(f\"Starting ingestion of {len(entities)} entities...\")\n",
        "\n",
        "batch_size = 100\n",
        "results = {\n",
        "    \"total_entities\": len(entities),\n",
        "    \"created\": 0,\n",
        "    \"updated\": 0,\n",
        "    \"failed\": 0,\n",
        "    \"errors\": []\n",
        "}\n",
        "\n",
        "for i in range(0, len(entities), batch_size):\n",
        "    batch = entities[i:i + batch_size]\n",
        "    batch_num = (i // batch_size) + 1\n",
        "    total_batches = (len(entities) + batch_size - 1) // batch_size\n",
        "    \n",
        "    logger.info(f\"Processing batch {batch_num}/{total_batches} ({len(batch)} entities)...\")\n",
        "    \n",
        "    try:\n",
        "        # Bulk create entities\n",
        "        result = purview_client.bulk_create_entities(batch)\n",
        "        \n",
        "        created = result.get('entities_created', len(batch))\n",
        "        results['created'] += created\n",
        "        \n",
        "        logger.info(f\"✓ Batch {batch_num} completed: {created} entities created\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"✗ Batch {batch_num} failed: {e}\")\n",
        "        results['failed'] += len(batch)\n",
        "        results['errors'].append({\n",
        "            \"batch\": batch_num,\n",
        "            \"error\": str(e)\n",
        "        })\n",
        "\n",
        "logger.info(\"Ingestion complete\")\n",
        "logger.info(f\"  Created: {results['created']}\")\n",
        "logger.info(f\"  Failed: {results['failed']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display results\n",
        "print(\"=\" * 60)\n",
        "print(\"Purview Ingestion Results\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Total Entities: {results['total_entities']}\")\n",
        "print(f\"Successfully Created: {results['created']}\")\n",
        "print(f\"Failed: {results['failed']}\")\n",
        "\n",
        "if results['errors']:\n",
        "    print(f\"\\nErrors ({len(results['errors'])}):\")\n",
        "    for error in results['errors'][:5]:  # Show first 5 errors\n",
        "        print(f\"  Batch {error['batch']}: {error['error']}\")\n",
        "\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save ingestion results\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "results_path = f\"/lakehouse/default/Files/logs/ingestion_{timestamp}.json\"\n",
        "\n",
        "results_data = {\n",
        "    \"timestamp\": timestamp,\n",
        "    \"atlas_json_path\": atlas_json_path,\n",
        "    \"purview_endpoint\": purview_endpoint,\n",
        "    \"results\": results,\n",
        "    \"metadata\": metadata\n",
        "}\n",
        "\n",
        "with open(results_path, 'w') as f:\n",
        "    json.dump(results_data, f, indent=2, default=str)\n",
        "\n",
        "logger.info(f\"Results saved to: {results_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Archive processed Atlas JSON if successful\n",
        "if results['failed'] == 0:\n",
        "    archive_path = atlas_json_path.replace('/processed/', '/archive/')\n",
        "    \n",
        "    import shutil\n",
        "    shutil.move(atlas_json_path, archive_path)\n",
        "    \n",
        "    logger.info(f\"Atlas JSON archived to: {archive_path}\")\n",
        "else:\n",
        "    # Move to errors folder for investigation\n",
        "    error_path = atlas_json_path.replace('/processed/', '/errors/')\n",
        "    \n",
        "    import shutil\n",
        "    shutil.move(atlas_json_path, error_path)\n",
        "    \n",
        "    logger.warning(f\"Atlas JSON moved to errors: {error_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Return output\n",
        "output = {\n",
        "    \"status\": \"success\" if results['failed'] == 0 else \"partial\",\n",
        "    \"entities_created\": results['created'],\n",
        "    \"entities_failed\": results['failed'],\n",
        "    \"results_path\": results_path,\n",
        "    \"timestamp\": timestamp\n",
        "}\n",
        "\n",
        "logger.info(\"Purview ingestion workflow complete\")\n",
        "logger.info(f\"Output: {json.dumps(output, indent=2)}\")\n",
        "\n",
        "mssparkutils.notebook.exit(json.dumps(output))"
      ]
    }
  ]
}
