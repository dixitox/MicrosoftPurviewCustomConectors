{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Metadata Extractor for Microsoft Purview\n",
        "\n",
        "This notebook extracts metadata from data sources and prepares it for ingestion into Purview.\n",
        "\n",
        "**Parameters:**\n",
        "- `source_type`: Type of data source (sql_server, postgresql, file_system, etc.)\n",
        "- `connection_name`: Name of the connection or connection string\n",
        "- `collection_name`: Purview collection name\n",
        "- `use_gateway`: Whether to use on-premises data gateway (true/false)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from notebookutils import mssparkutils\n",
        "from datetime import datetime\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get parameters from pipeline\n",
        "source_type = mssparkutils.notebook.getArgument(\"source_type\", \"sql_server\")\n",
        "connection_name = mssparkutils.notebook.getArgument(\"connection_name\", \"\")\n",
        "collection_name = mssparkutils.notebook.getArgument(\"collection_name\", \"default\")\n",
        "use_gateway = mssparkutils.notebook.getArgument(\"use_gateway\", \"false\").lower() == \"true\"\n",
        "\n",
        "logger.info(f\"Source Type: {source_type}\")\n",
        "logger.info(f\"Connection: {connection_name}\")\n",
        "logger.info(f\"Collection: {collection_name}\")\n",
        "logger.info(f\"Use Gateway: {use_gateway}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install pyapacheatlas azure-identity python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import Purview SDK (assuming it's uploaded to workspace or installed)\n",
        "import sys\n",
        "sys.path.append(\"/lakehouse/default/Files/libs\")  # If SDK uploaded to Lakehouse\n",
        "\n",
        "from purview_connector_sdk import PurviewClient, DatabaseConnector, FileSystemConnector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Purview client using Managed Identity\n",
        "logger.info(\"Initializing Purview client...\")\n",
        "\n",
        "# Get Purview endpoint from environment or Key Vault\n",
        "try:\n",
        "    purview_endpoint = mssparkutils.credentials.getSecret(\n",
        "        \"purview-connector-kv\", \n",
        "        \"purview-endpoint\"\n",
        "    )\n",
        "except:\n",
        "    # Fallback to environment variable\n",
        "    purview_endpoint = \"https://your-purview.purview.azure.com\"\n",
        "\n",
        "purview_client = PurviewClient(\n",
        "    endpoint=purview_endpoint,\n",
        "    use_managed_identity=True\n",
        ")\n",
        "\n",
        "logger.info(f\"Connected to Purview: {purview_endpoint}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract metadata based on source type\n",
        "logger.info(f\"Extracting metadata from {source_type}...\")\n",
        "\n",
        "if source_type == \"sql_server\":\n",
        "    # Database connector\n",
        "    connector = DatabaseConnector(\n",
        "        purview_client=purview_client,\n",
        "        source_type=source_type,\n",
        "        connection_string=connection_name,  # Or connection config\n",
        "        use_gateway=use_gateway,\n",
        "        collection_name=collection_name\n",
        "    )\n",
        "    \n",
        "elif source_type == \"file_system\":\n",
        "    # File system connector\n",
        "    connector = FileSystemConnector(\n",
        "        purview_client=purview_client,\n",
        "        root_path=connection_name,\n",
        "        recursive=True,\n",
        "        use_gateway=use_gateway,\n",
        "        collection_name=collection_name\n",
        "    )\n",
        "    \n",
        "else:\n",
        "    raise ValueError(f\"Unsupported source type: {source_type}\")\n",
        "\n",
        "# Extract metadata\n",
        "metadata = connector.extract_metadata()\n",
        "logger.info(f\"Extracted metadata successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display metadata summary\n",
        "print(\"=\" * 60)\n",
        "print(\"Metadata Extraction Summary\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if source_type == \"sql_server\":\n",
        "    print(f\"Database: {metadata.get('database_name')}\")\n",
        "    schemas = metadata.get('schemas', [])\n",
        "    print(f\"Schemas: {len(schemas)}\")\n",
        "    \n",
        "    total_tables = sum(len(s.get('tables', [])) for s in schemas)\n",
        "    print(f\"Total Tables: {total_tables}\")\n",
        "    \n",
        "    total_columns = sum(\n",
        "        len(t.get('columns', [])) \n",
        "        for s in schemas \n",
        "        for t in s.get('tables', [])\n",
        "    )\n",
        "    print(f\"Total Columns: {total_columns}\")\n",
        "    \n",
        "elif source_type == \"file_system\":\n",
        "    print(f\"Root Path: {metadata.get('root_path')}\")\n",
        "    print(f\"Files: {len(metadata.get('files', []))}\")\n",
        "    print(f\"Directories: {len(metadata.get('directories', []))}\")\n",
        "\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save metadata to Lakehouse\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "output_path = f\"/lakehouse/default/Files/staging/metadata_{source_type}_{timestamp}.json\"\n",
        "\n",
        "logger.info(f\"Saving metadata to: {output_path}\")\n",
        "\n",
        "# Convert to JSON and save\n",
        "metadata_json = json.dumps(metadata, indent=2, default=str)\n",
        "\n",
        "with open(output_path, 'w') as f:\n",
        "    f.write(metadata_json)\n",
        "\n",
        "logger.info(\"Metadata saved successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save execution log\n",
        "log_path = f\"/lakehouse/default/Files/logs/extraction_{timestamp}.log\"\n",
        "\n",
        "log_entry = {\n",
        "    \"timestamp\": timestamp,\n",
        "    \"source_type\": source_type,\n",
        "    \"connection_name\": connection_name,\n",
        "    \"status\": \"success\",\n",
        "    \"metadata_path\": output_path,\n",
        "    \"entity_count\": total_tables if source_type == \"sql_server\" else len(metadata.get('files', []))\n",
        "}\n",
        "\n",
        "with open(log_path, 'w') as f:\n",
        "    json.dump(log_entry, f, indent=2)\n",
        "\n",
        "logger.info(f\"Log saved to: {log_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Return output for next activity in pipeline\n",
        "output = {\n",
        "    \"status\": \"success\",\n",
        "    \"metadata_path\": output_path,\n",
        "    \"log_path\": log_path,\n",
        "    \"timestamp\": timestamp,\n",
        "    \"entity_count\": log_entry[\"entity_count\"]\n",
        "}\n",
        "\n",
        "logger.info(\"Metadata extraction complete\")\n",
        "logger.info(f\"Output: {json.dumps(output, indent=2)}\")\n",
        "\n",
        "# Exit notebook with output\n",
        "mssparkutils.notebook.exit(json.dumps(output))"
      ]
    }
  ]
}
